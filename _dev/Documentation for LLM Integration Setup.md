## First Steps
1. Install Python from `python.org`
2. Check Python install with `py.exe --version`
3. Install `ollama` from their website, allows running of LLMs locally.
4. Pull with bash the `llama3` model with the command: `ollama pull llama3`
5. Or we can also pull `gemma3` with `ollama pull gemma3`
6. For whatever reason, I found that running with the Mingw64 console and not the native windows terminal 

